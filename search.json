[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Yo, Rushi here! I recently graduated from IIT Jodhpur. Most of my time is spent researching and developing artificial learning algorithms inspired by the incredible human brain. I like to learn new things by helping others learn about them. I love explaining complex ideas in a simple manner and simple concepts with a deeper perspective!\nAlso, e/acc ftw! Let’s make sure AGI takes over the world before politics and war do!"
  },
  {
    "objectID": "posts/probability_refresher/index.html",
    "href": "posts/probability_refresher/index.html",
    "title": "An Expansive Probability Refresher",
    "section": "",
    "text": "Many students study probability in high school or college but fail to grasp the common-sensical understanding of this natural topic. They usually end up memorising a ton of formulae and applying them to a hundred questions. I don’t know about them, but probability surely deserves better! I will explain everything in granular detail, providing informal, simple explanations alongside formal definitions of various terms and concepts. So, without further ado, let’s start!"
  },
  {
    "objectID": "posts/probability_refresher/index.html#what-is-a-random-variable",
    "href": "posts/probability_refresher/index.html#what-is-a-random-variable",
    "title": "An Expansive Probability Refresher",
    "section": "What is a random variable?",
    "text": "What is a random variable?\nIt’s just a function that maps an event’s textual description to a relevant number. For example, if we toss a coin, a random variable \\(X\\) can be defined as follows:\n\\[X(heads) = 1\\]\nNow, let’s toss two coins and define \\(X\\) to denote the number of heads as shown below:\n\\[X(TT) = 0\\]\nSo, apparently, the only reason random variables exist is that our X-axis can’t fit words :))! Quite some existential crisis, isn’t it?"
  },
  {
    "objectID": "posts/probability_refresher/index.html#what-exactly-is-probability",
    "href": "posts/probability_refresher/index.html#what-exactly-is-probability",
    "title": "An Expansive Probability Refresher",
    "section": "What exactly is probability?",
    "text": "What exactly is probability?\nLet’s start with the most basic definition of probability. \\[probability = \\frac {number \\, of \\, favourable \\, outcomes}{total \\, number \\, of \\, possible \\, outcomes}\\] This can be applied to the simple example of a coin toss using the random variable \\(X\\)as shown above, as follows: \\[prob(X=1) = \\frac {1} {2} \\, ; \\, prob(X=0) = \\frac {1} {2}\\] However, this definition has two significant limitations :(!\n\nThe EQ Problem!\nOur definition works well for the unbiased coin toss example. But, let’s take a biased coin with heads on both sides(like the one shown in the movie “Sholay”!). According to our formula: \\[prob(X=1) = \\frac {num \\, outcomes \\, having \\, heads} {total \\, number \\, of \\, outcomes} = \\frac {1}{2}\\] However, in reality, \\(prob(X=1) = 1\\). Hence, our definition does not work when all outcomes are not EQually likely. In such cases, we conduct experiments and note down the outcomes. This gives us the evidence to support our probability values. Hence, the new definition of probability becomes: \\[prob(X=\\lambda) = \\lim\\limits_{n_{total} \\rightarrow \\infty} \\frac {n_\\lambda} {n_{total}}\\] Let’s understand this with the typical example of a coin toss. Suppose we are dumb and don’t know that \\(prob(heads) = 0.5\\). However, we are not too dumb, so we decided to do ten coin tosses and observe. We get three heads and seven tails. So, we define \\(prob(heads) = 0.3\\). Now, a moderately clever (but idle!) person says, “Let’s do it 1000 times!”. This time, we get 550 heads and 450 tails. Now, \\(prob(heads)\\) becomes \\(0.55\\). As you can notice, as we increase the number of tosses, our probabilities get closer and closer to the correct answer. This is the logic behind the \\(n_{total} \\rightarrow \\infty\\) limit in the above formula.\n\n\nThe Infinity Problem\nIt only works when the total number of possible outcomes is finite. This is a big problem because it is not the case in most practical scenarios. For example, what can we do if we want to find the value of \\(prob(temp \\in [17.29^{\\circ}C, 20^{\\circ}C])\\) in a room?\nCurrently, we are viewing probability as just discrete values assigned to a finite number of events. However, think about it and tell me if going around assigning probabilities to each and every point on the x-axis is a good option or not. Of course, it’s foolish. Let’s try peeking through the lens of calculus. One of the main lessons of calculus was to “think in terms of ranges rather than points”. Okay, so we make a graph where the x-axis contains the infinite range of values available to us while the y-axis contains \\(prob(X \\le x)\\). Let’s name this function of this graph as \\(cprob\\). Hence, rather than focussing on \\(prob(X=x)\\), we focus on \\(prob(X \\le x)\\). This way, the problem is solved because: \\[\\small \\begin{equation*} \\begin{split} prob(temp \\in [17.29^{\\circ}C, 20^{\\circ}C]) &= prob(temp \\le 20^{\\circ}C) - prob(temp \\le 17.29^{\\circ}C) \\\\ &= cprob(temp=20^{\\circ}C) - cprob(temp=17.29^{\\circ}C) \\end{split} \\\\ \\end{equation*}\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#cdf",
    "href": "posts/probability_refresher/index.html#cdf",
    "title": "An Expansive Probability Refresher",
    "section": "CDF",
    "text": "CDF\nA cumulative distribution function is a function \\(F_X : \\mathbb{R} \\rightarrow [0, 1]\\) which specifies a probability measure as: \\[F_X(x) = P(X \\le x)\\] Some properties of CDF include:\n\nIt is an increasing function.\n\\(lim_{x \\rightarrow -\\infty} F_X(x) = 0\\) as \\(P(\\phi) = 0\\)\n\\(lim_{x \\rightarrow \\infty} F_X(x) = 1\\) as \\(P(\\varOmega) = 1\\)"
  },
  {
    "objectID": "posts/probability_refresher/index.html#pmf",
    "href": "posts/probability_refresher/index.html#pmf",
    "title": "An Expansive Probability Refresher",
    "section": "PMF",
    "text": "PMF\nFor discrete random variables, it is simpler to literally define the probability for each point. This is called the probability mass function of that random variable. For example, let us consider the tossing of two coins. The random variable is the number of heads, i.e. \\(X(\\varOmega) \\in Val(X) = \\{ 0, 1, 2 \\}\\). So, the PMF \\(p_X : Val(X) \\rightarrow [0, 1]\\) can be defined as follows: \\[p_X(0) = 0.25\\] \\[p_X(1) = 0.5\\] \\[p_X(2) = 0.25\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#pdf",
    "href": "posts/probability_refresher/index.html#pdf",
    "title": "An Expansive Probability Refresher",
    "section": "PDF",
    "text": "PDF\nIf a CDF is continuous and differentiable everywhere, the probability density function is defined as the derivative of CDF. \\[f_X(x) = \\frac{dF_X(x)}{dx}\\] Some subtle points about PDFs are as follows:\n\nPDF does not exist for a random variable if its CDF is continuous but not differentiable everywhere.\nValue of PDF at a given point \\(x\\) is not necessarily equal to the probability of that event, i.e.,\\[f_X(x) \\ne P(X=x)\\]In fact, for continuous random variables, \\(P(X=x) = 0 \\enspace \\forall \\enspace x\\) because we only talk about interval probabilities whenever continuous random variables are concerned.\n\\(f_X(x) \\ge 0\\), but not necessarily \\(\\le 1\\), as the derivative can be greater than \\(1\\).\n\\(\\int_{- \\infty}^{\\infty} f_X(x) = 1\\)"
  },
  {
    "objectID": "posts/probability_refresher/index.html#expectation",
    "href": "posts/probability_refresher/index.html#expectation",
    "title": "An Expansive Probability Refresher",
    "section": "Expectation",
    "text": "Expectation\nIf \\(p_X(x)\\) is the PMF of a discrete variable \\(X\\), \\[E[g(X)] = \\sum_{x \\in Val(X)} g(x)p_X(x)\\] On the other hand, for a continuous variable \\(X\\) with PDF \\(f_X(x)\\), \\[E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)\\]\nSome properties of expectations are as follows: * \\(E[X]\\) is the same as \\(mean(X)\\) conceptually. It’s just that \\(E[X]\\) is used to define what will happen in the future while \\(mean(X)\\) is used to “summarise” the past. * \\(E[1\\{ X=k \\}] = P(X=k)\\)"
  },
  {
    "objectID": "posts/probability_refresher/index.html#variance",
    "href": "posts/probability_refresher/index.html#variance",
    "title": "An Expansive Probability Refresher",
    "section": "Variance",
    "text": "Variance\nIt is the measure of how concentrated a distribution is around its mean. Mathematically, we can express it as: \\[Var[X] = E[(X-E[X])^2]\\] Also, one more way to write the above formula would be: \\[Var[X] = E[X^2] - (E[X])^2\\] A simple property of variance is: \\[Var[aX] = a^2Var[X]\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#the-discrete-distros",
    "href": "posts/probability_refresher/index.html#the-discrete-distros",
    "title": "An Expansive Probability Refresher",
    "section": "The Discrete Distros",
    "text": "The Discrete Distros\n\nBernoulli - Probability of getting a head in a simple coin toss!\\[\\small p_X(x) = \\begin{cases} p &\\text{if } x = 1 \\\\ 1 - p &\\text{if } x = 0 \\end{cases}\\]\n\n\n\n\n\n\n\nBinomial - Probability of getting \\(x\\) heads in \\(n\\) tosses!\\[\\small p_X(x) = {n \\choose x} . p^x (1-p)^{n-x}\\]\n\n\n\n\n\n\n\nGeometric - Probability of getting the first head at the \\(x^{th}\\) toss!\\[p_X(x) = p(1-p)^{x-1}\\]\n\n\n\n\n\n\n\nPoisson - It is just a special case of the binomial distribution but requires some further explanation. In binomial, we know the number of trials and the probability of success in a particular trial. However, imagine a situation where we know the average success rate over a given period. Let this rate of success be \\(\\lambda\\). Also, the number of trials \\(n\\) would tend to \\(\\infty\\). Put \\(p = \\frac {\\lambda} {n}\\) in the binomial formula. After solving the limit on \\(n \\rightarrow \\infty\\) (full proof here), we get the following formula to find out the probability of getting \\(k\\) successes per period (the same as \\(\\lambda\\)’s period) would be:\\[p_X(X=k) = \\frac {e^{-\\lambda} \\lambda^k} {k!}\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#the-continuous-distros",
    "href": "posts/probability_refresher/index.html#the-continuous-distros",
    "title": "An Expansive Probability Refresher",
    "section": "The Continuous Distros",
    "text": "The Continuous Distros\n\nUniform\\[\\small f_X(x) = \\begin{cases} \\frac {1}{b-a} &\\text{if } a \\le x \\le b \\\\ 0 &\\text{otherwise} \\end{cases}\\]\n\n\n\n\n\n\n\nExponential\\[\\small f_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} &\\text{if } x \\ge 0 \\\\ 0 &\\text{otherwise} \\end{cases}\\]\n\n\n\n\n\n\n\nGaussian\\[f_X(x) = \\frac {1} {\\sqrt{2 \\pi} \\ \\sigma} e^{ \\frac{1}{2} (\\frac {x-\\mu}{\\sigma})^2}\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#joint-cdf-and-marginal-cdf",
    "href": "posts/probability_refresher/index.html#joint-cdf-and-marginal-cdf",
    "title": "An Expansive Probability Refresher",
    "section": "Joint CDF and Marginal CDF",
    "text": "Joint CDF and Marginal CDF\nWorking with two variables means analysing their probability distribution together. Let our variables be \\(X\\) and \\(Y\\). Then, the joint cumulative function can be quite analogously defined as: \\[F_{XY} (x, y) = P(X \\le x, Y \\le y)\\] Great! But, we may also want to observe just one variable. Given the joint CDF, we can easily do that as follows: \\[F_{X} (x) = \\lim\\limits_{y \\rightarrow \\infty} F_{XY} (x, y)\\] These are called the marginal cumulative distribution functions. Here, the logic behind \\(y \\rightarrow \\infty\\) and \\(x \\rightarrow \\infty\\) is to allow all values of that variable, which ensures that the existence of that variable is being fully considered.\nSome properties of joint CDF include:\n\n\\(\\lim\\limits_{x, y \\rightarrow -\\infty} F_{XY} (x, y) = 0\\)\n\\(\\lim\\limits_{x, y \\rightarrow \\infty} F_{XY} (x, y) = 1\\)"
  },
  {
    "objectID": "posts/probability_refresher/index.html#joint-pmf-and-marginal-pmf",
    "href": "posts/probability_refresher/index.html#joint-pmf-and-marginal-pmf",
    "title": "An Expansive Probability Refresher",
    "section": "Joint PMF and Marginal PMF",
    "text": "Joint PMF and Marginal PMF\nFor two discrete random variables \\(X\\) and \\(Y\\), we can simply define the joint probability mass function as: \\[p_{XY}(x, y) = P(X=x, Y=y)\\] Let the set of all possible values taken by \\(X\\) and \\(Y\\) be \\(Val(X)\\) and \\(Val(Y)\\). Quite easily, the marginal probability mass function can be said to be: \\[p_X(x) = \\sum_{y \\in Val(Y)} p_{XY}(x, y)\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#joint-pdf-and-marginal-pdf",
    "href": "posts/probability_refresher/index.html#joint-pdf-and-marginal-pdf",
    "title": "An Expansive Probability Refresher",
    "section": "Joint PDF and Marginal PDF",
    "text": "Joint PDF and Marginal PDF\nIf the joint CDF is differentiable everywhere, we can define the joint probability density function as follows: \\[f_{XY}(x, y) = \\frac {\\partial^2 F_{XY}(x, y)} {\\partial x \\partial y}\\] Similarly, we can define the marginal probability density function as: \\[f_X(x, y) = \\int_{-\\infty}^{\\infty} f_{XY}(x, y)dy\\] Also, \\[\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{XY} (x, y) dxdy = 1\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#expectation-and-covariance",
    "href": "posts/probability_refresher/index.html#expectation-and-covariance",
    "title": "An Expansive Probability Refresher",
    "section": "Expectation and Covariance",
    "text": "Expectation and Covariance\nFor discrete variables, \\[\\mathbb{E}[g(X, Y)] = \\sum_{x \\in Val(X)} \\sum_{y \\in Val(Y)} g(x, y)p_{XY}(x, y)\\] For continuous variables, \\[\\mathbb{E}[g(X, Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y)f_{XY}(x, y)dxdy\\] Now, a useful thing to observe is how both variables “correlate” with each other. By “correlate”, I mean “do larger values of \\(X\\) correspond with larger values of \\(Y\\) and how often?”. This is called covariance and can be computed as follows: \\[Cov[X, Y] = \\mathbb{E} [(X - \\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\\] By a little manuevering, we can derive: \\[Cov[X, Y] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\] When larger values of \\(X\\) correspond with larger values of \\(Y\\), the covariance is negative. On the other hand, when larger values of \\(X\\) correspond with smaller values of \\(Y\\), the covariance is positive.\nSome important points to note:\n\n\\(Var[X + Y] = Var[X] + Var[Y] + 2Cov[X, Y]\\)\n\\(Cov[X, Y]=0 \\rightarrow\\) Independence of \\(X\\) and \\(Y\\) but its inverse is not true. A counter-example is: Take \\(X\\) and \\(Y\\) such that \\(Y=X^2\\). Here, \\(Cov[X, Y] = 0\\) but \\(X\\) and \\(Y\\) are clearly not independent."
  },
  {
    "objectID": "posts/probability_refresher/index.html#conditional-probability",
    "href": "posts/probability_refresher/index.html#conditional-probability",
    "title": "An Expansive Probability Refresher",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nLet \\(A\\) and \\(B\\) be two events. The probability of an event \\(A\\) after observing the occurrence of the event \\(B\\) is termed as conditional probability and denoted by \\(P(A | B)\\). If \\(P(B) \\ne 0\\), we can define the conditional probability as: \\[P(A | B) = \\frac {P(A \\cap B)}{P(B)}\\] The logic behind this famous problem is not that famous, unfortunately! Let me explain. Let \\(S\\) be the sample space. Our task is to find the probability of \\(A\\) given that we have observed \\(B\\). Obviously, \\(P(S) = 1\\). However, we need to focus on \\(B\\) which is a subset of \\(S\\). So, we need to define another function \\(P^\\prime\\) such that \\(P^\\prime (B) = 1\\). This leads us to the following definition of \\(P^\\prime\\): \\[P^\\prime(A) = \\frac {1}{P(B)} P(A)\\] However, there’s a small catch. Since we have observed \\(B\\), we need to discard the part of \\(A\\) which resides outside of \\(B\\)! Hence, the new definition becomes: \\[P^\\prime(A) = \\frac {1}{P(B)} P(A \\cap B)\\]"
  },
  {
    "objectID": "posts/probability_refresher/index.html#the-concept-of-independence",
    "href": "posts/probability_refresher/index.html#the-concept-of-independence",
    "title": "An Expansive Probability Refresher",
    "section": "The Concept of Independence",
    "text": "The Concept of Independence\nOnly a bit of common sense is required to understand that \\(A\\) and \\(B\\) are independent if: \\[P(A|B) = P(A)\\] Putting in the conditional probability formula, we get: \\[P(A \\cap B) = P(A) \\cdot P(B)\\]"
  },
  {
    "objectID": "posts/policy_gradient/index.html",
    "href": "posts/policy_gradient/index.html",
    "title": "The Policy Gradient",
    "section": "",
    "text": "The idea of policy-gradient methods is to change our policy using gradient descent methods such that our RL objective is maximised. Let’s see how.\n(Feel free to check out my previous blog to get an introduction to RL, if you are not familiar with the basic ideology.)"
  },
  {
    "objectID": "posts/policy_gradient/index.html#causality-assumption",
    "href": "posts/policy_gradient/index.html#causality-assumption",
    "title": "The Policy Gradient",
    "section": "Causality Assumption",
    "text": "Causality Assumption\nThis assumption says that “the present policy cannot affect previous rewards.” Come on man, this is always true, provided that we don’t do time travel. Also, note that this is not the same as the Markov property, which says that “the states in the future are independent of the past given the present.”\nWe can remove some unnecessary terms from our gradient estimation by using this “causality” assumption as follows:\n\\[\n\\begin{equation*}\n\\begin{split}\n\\nabla_\\theta J(\\theta) &\\approx \\frac{1}{N} \\sum_{i=1}^N  \\bigg( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta (a_{i,t} | s_{i,t}) \\bigg) \\bigg( \\sum_{t=1}^Tr(s_{i,t}, a_{i,t}) \\bigg)\n\\\\\n&= \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta (a_{i,t} | s_{i,t}) \\bigg( \\sum_{t'=1}^Tr(s_{i,t'}, a_{i,t'}) \\bigg)\n\\\\\n&= \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta (a_{i,t} | s_{i,t}) \\bigg( \\sum_{t'=\\textcolor{red}{t}}^Tr(s_{i,t'}, a_{i,t'}) \\bigg)\n\\end{split}\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "posts/policy_gradient/index.html#baseline-trick",
    "href": "posts/policy_gradient/index.html#baseline-trick",
    "title": "The Policy Gradient",
    "section": "Baseline Trick",
    "text": "Baseline Trick\nRemember the intuition behind REINFORCE? The idea was to “change the policy such that the probability of trajectories with higher rewards and vice-versa”. What if we modify this statement slightly? The new idea is to “change the policy such that the probability of trajectories with rewards higher than average and vice-versa”. This is better, isn’t it? So, the new gradient becomes:\n\\[\n\\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta log(p_\\theta(\\tau_i)) \\cdot (r(\\tau_i) \\textcolor{red}{- b})\n\\\\\n\\text{where } b=\\frac{1}{N} \\sum_{i=1}^N r(\\tau_i)\n\\]\nBut …\n\nIs this new estimator still unbiased?\nYes! Because the expectation of \\(E[\\nabla_\\theta log(p_\\theta(\\tau)) \\cdot b]\\) is zero. Let’s see how:\n\\[\n\\begin{equation*}\n\\begin{split}\nE[\\nabla_\\theta log(p_\\theta(\\tau)) \\cdot b]\n&= \\int p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau)b d\\tau\n\\\\\n&= \\int \\nabla_\\theta p_\\theta(\\tau)b d\\tau\n\\\\\n&= b \\nabla_\\theta \\int p_\\theta(\\tau) d\\tau\n\\\\\n&= b \\nabla_\\theta 1\n\\\\\n&= 0\n\\end{split}\n\\end{equation*}\n\\]\nThis means that the addition of \\(b\\) doesn’t affect the bias of the old estimator. Good.\n\n\nDoes this new estimator have lower variance?\nFor the sake of notational simplicity, let’s keep \\(g(\\tau) = \\nabla_\\theta log(p_\\theta(\\tau))\\). Let’s calculate the variance now:\n\\[\n\\nabla_\\theta J(\\theta) = E_{\\tau\\sim p(\\tau)} [g(\\tau) (r(\\tau) - b)]\n\\]\nUsing the identity \\(Var(X) = E[X^2] - E[X]^2\\), we can write:\n\\[\n\\begin{equation*}\n\\begin{split}\n\\text{New Var} &= E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 (r(\\tau) - b)^2] - E[g(\\tau) (r(\\tau) - b)]^2\n\\\\\n&= E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 (r(\\tau) - b)^2] - E[g(\\tau) (r(\\tau))]^2 \\text{ using previous result}\n\\\\\n&= E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)^2] - 2bE_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] + b^2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2] - E[g(\\tau) (r(\\tau))]^2\n\\\\\n&= \\textcolor{red}\\{E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)^2]  - E[g(\\tau) (r(\\tau))]^2 \\textcolor{red}\\} - 2bE_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] + b^2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2]\n\\\\\n&= \\text{Old Var} - 2bE_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] + b^2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2]\n\\end{split}\n\\end{equation*}\n\\]\nNow, if we want the baseline to reduce the variance, the new variance has to be smaller.\n\\[\n\\text{New Var} - \\text{Old Var} &lt; 0\n\\\\\n- 2bE_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] + b^2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2] &lt; 0\n\\]\nAssuming positive rewards, we can say that \\(b&gt;0\\). This means that:\n\\[\n0 &lt; b &lt; \\frac{2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] }{ E_{\\tau\\sim p(\\tau)} [g(\\tau)^2]}\n\\]\nOur baseline \\(b = E_{\\tau\\sim p(\\tau)} [r(\\tau)] \\approx \\frac{1}{N} \\sum_{i=1}^N r(\\tau_i)\\) will satisfy this condition only if \\(cov[(g(\\tau)^2, r(\\tau)] &lt; -E[g(\\tau)^2 \\cdot r(\\tau)]\\) which is reasonable since the magnitude of gradient of log probs is more likely to be positively correlated with the rewards than not (maybe not that likely in the beginning but definitely in the later stages). This has been supported by experiments in practical scenarios.\n\n\nIs “average” the best (no pun intended!) possible option? Can we do better?\nYes. Let’s calculate the optimal value of the baseline by minimising the \\(\\text{New Var}\\).\n\\[\\begin{equation*}\n\\begin{split}\n\\text{New Var} &= \\text{Old Var} - 2bE_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] + b^2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2]\n\\\\\n\\frac{d\\text{ New Var}}{db} &=  - 2E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)] + 2bE_{\\tau\\sim p(\\tau)} [g(\\tau)^2] = 0\n\\\\\nb &= \\frac{E_{\\tau\\sim p(\\tau)} [g(\\tau)^2 r(\\tau)]}{E_{\\tau\\sim p(\\tau)} [g(\\tau)^2]}\n\\end{split}\n\\end{equation*}\\]\nSo, so, so, the baseline depends on \\(g(\\tau)\\) which might make it slightly compute-intensive. So, using “average” as the baseline can be a efficient yet good option. Depends on the computational constraints of the problem.\nThanks for reading! Hope you liked it. If you find any mistakes or inaccuracies, feel free to send an email to shah.15@iitj.ac.in."
  },
  {
    "objectID": "posts/kolmogorov_and_soph/index.html",
    "href": "posts/kolmogorov_and_soph/index.html",
    "title": "Kolmogorov Complexity and “Sophistication”",
    "section": "",
    "text": "It seems to me that the most important discovery since Gödel was the discovery by Chaitin, Solomonoff and Kolmogorov of the concept called Algorithmic Probability which is a fundamental new theory of how to make predictions given a collection of experiences and this is a beautiful theory, everybody should learn it, but it’s got one problem, that is, that you cannot actually calculate what this theory predicts because it is too hard, it requires an infinite amount of work. However, it should be possible to make practical approximations to the Chaitin, Kolmogorov, Solomonoff theory that would make better predictions than anything we have today. Everybody should learn all about that and spend the rest of their lives working on it.\n— Marvin Minsky in a panel discussion on The Limits of Understanding, World Science Festival, NYC, 2014\nCredits: Wikipedia article on Minimum Description Length\n\nRecently, I read this great blog called “The First Law of Complexodynamics” by Scott Aaronson, which I found in Ilya Sutskever’s famous paper list. Even after reading it several times, I couldn’t get my dumb head around the definition of “sophistication”, which is an important part of that blog. I finally understood it, and it is quite cool. So, I decided to have a go at explaining it in my own words. Note that even though the writings that follow are self-sufficient and demand no prerequisites, I encourage you to have a go at Scott’s blog first and get a hold of the bigger picture."
  },
  {
    "objectID": "posts/kolmogorov_and_soph/index.html#why-not-kc",
    "href": "posts/kolmogorov_and_soph/index.html#why-not-kc",
    "title": "Kolmogorov Complexity and “Sophistication”",
    "section": "Why not KC?",
    "text": "Why not KC?\nThe notion of Kolmogorov complexity is very helpful but it doesn’t quite fit into a particular interpretation of complexity which regards randomness as “not exactly complex”. A random string’s KC will be high even though it is not exactly complex, intuitively speaking. So, now we need a new measure which is low for simple strings as well as random strings but high for strings containing some inherent patterns."
  },
  {
    "objectID": "posts/kolmogorov_and_soph/index.html#sophistication",
    "href": "posts/kolmogorov_and_soph/index.html#sophistication",
    "title": "Kolmogorov Complexity and “Sophistication”",
    "section": "Sophistication",
    "text": "Sophistication\nGiven a string \\(x\\), amongst all possible sets obeying the below conditions, the KC of the set which has the lowest KC is termed as the sophistication \\(Soph(x)\\).\n\n\\(x \\in S\\)\n\\(K(x|S) \\ge \\log_2(|S|) - c\\), for some constant \\(c \\ge 0\\). (In other words, one can distill all the “nonrandom” information in x just by saying that \\(x\\) belongs that \\(S\\).)\n\nIn mathematical terms,\n\\[\nS^* = \\arg\\min_{S} \\{ K(S) \\mid \\text{cond 1 and cond 2 holds} \\}\n\\\\\n\\text{Soph}(x) = K(S^*)\n\\]\nI know you’re like “woah wait, what the hell are these conditions, especially the second one?”. I had the same reaction but trust me, they’re quite cool!"
  },
  {
    "objectID": "posts/kolmogorov_and_soph/index.html#explaining-condition-2",
    "href": "posts/kolmogorov_and_soph/index.html#explaining-condition-2",
    "title": "Kolmogorov Complexity and “Sophistication”",
    "section": "Explaining Condition 2",
    "text": "Explaining Condition 2\nThe first condition is trivial, but the second one is fairly difficult to digest. The main question I had was “how does the 2nd condition make \\(Soph(x)\\) low for both simple strings and random strings and high for strings containing patterns?”.\nLet’s tackle the “highly simple string” scenario first. In this case, the valid set having lowest KC i.e. \\(S^* = ​\\set{x}\\). Wait what? But is it even a valid set? Yes it is. It satisfies both conditions:\n\n\\(x \\in \\set x\\).\nSince \\(|\\set x| = 1\\), condition 2 trivially holds.\n\nMoreover, \\(K(\\set x) = K(x) = \\text{a very small value}\\), since the string \\(x\\) is very simple, making it the valid set having the lowest KC. Hence,\n\\[\nS^* = \\set{x}\n\\\\\n\\text{Soph}(x) = K(S^*) = \\text{a very small value}\n\\]\nLet’s move onto the “highly random string” case. First of all, can we use the singleton set \\(\\set{x}\\) as \\(S^*\\) again? We can see that it is a valid set, but does it have the least KC? Oops. Not anymore. That’s because the string \\(x\\) is highly random making it a high-KC string. That means \\(K(\\set{x})\\) is also very high. All in all, while it is a valid set, it is not the one with the lowest KC.\nBut what is \\(S^*\\) then? Here’s comes the cool part. Our problem is that \\(S = \\set{x}\\) has a very high KC. If \\(x\\) is a very random string, we can directly start with \\(S = \\set{0, 1}^n\\). Is it valid? Yes, it is.\n\n\\(x \\in \\set{0, 1}^n\\)\nFor this one, we first need to find out the values of all the terms:\n\n\\(K(x|S) \\approx K(x) = n\\)\n\\(\\log_2(|\\set{0, 1}^n|) = \\log_2(2^n) = n\\)\n\nFor \\(c \\ge 0\\), \\(n \\ge n - c\\) and condition 2 is also satisfied. Viola!\n\nTherefore, while \\(\\set{x}\\) is a valid set satisfying the conditions, it doesn’t give us the minimum value of \\(K(S)\\) that the sophistication definition asks for. The set of all n-bit strings gives us a much smaller \\(K(S)\\) while still satisfying the conditions."
  },
  {
    "objectID": "posts/kolmogorov_and_soph/index.html#why-is-c-ge-0",
    "href": "posts/kolmogorov_and_soph/index.html#why-is-c-ge-0",
    "title": "Kolmogorov Complexity and “Sophistication”",
    "section": "Why is \\(c \\ge 0\\)?",
    "text": "Why is \\(c \\ge 0\\)?\nIf \\(c \\lt 0\\), the condition 2 becomes:\n\\[\nK(x|S) \\gt \\log_2(|S|)\n\\]\nFor any set \\(S\\) that contains \\(x\\), \\(K(x|S) \\le \\log_2(|S|)\\). I would leave this to the reader to think why. Hence, \\(c \\ge 0\\) to ensure that the condition is not too tight and impossible to satisfy."
  },
  {
    "objectID": "posts/kolmogorov_and_soph/index.html#what-is-the-significance-of-c",
    "href": "posts/kolmogorov_and_soph/index.html#what-is-the-significance-of-c",
    "title": "Kolmogorov Complexity and “Sophistication”",
    "section": "What is the significance of \\(c\\) ?",
    "text": "What is the significance of \\(c\\) ?\nIt is used to control the tightness of the constraint. High c means that the constraint is more lenient. The value of \\(K(x|S)\\) can be low i.e. we might find a set \\(S\\) with a lower KC that captures more and more of \\(x\\) and help Ada make her program much shorter. On the other hand, if \\(c\\) is very low, the constraint is tighter. The valid sets will be the ones that contain less and less information of \\(x\\).\nIn summary,\n\nToo small c: forces us to use larger, more generic sets\nToo large c: allows us to use very specific sets, defeating the purpose of measuring sophistication"
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html",
    "href": "posts/restricted_boltzmann_machines/index.html",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "",
    "text": "Generative modelling is the task of modelling the data distribution (which helps us generate new, similar, synthetic data points). There are multiple ways of dealing with this problem, with VAEs, diffusion models, and GANs being some of the popular ones. RBMs belong to this category of generative models, i.e. they are trained to model the data distribution \\(p(x)\\).\nIn this blog, we will extensively explore binary RBMs (i.e., RBMs that support only binary data). RBMs are not used today because training them on continuous data is difficult. Nevertheless, I find them to be a great introduction to energy-based models and concepts like Gibbs sampling and the Gibbs-Boltzmann distribution. Let’s start right away."
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#first-term-simplification",
    "href": "posts/restricted_boltzmann_machines/index.html#first-term-simplification",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "First Term Simplification",
    "text": "First Term Simplification\nNow, let’s calculate the first term of this gradient with \\(\\theta\\) as \\(w_{ij}\\):\n\\[\n-\\sum_\\textbf{h} p( \\textbf{h | v} ) \\frac{\\partial E(\\textbf{v}, \\textbf{h})}{\\partial w_{ij}}\n=\n\\sum_\\textbf{h} p( \\textbf{h | v} ) h_i v_j\n\\]\nHere, some calculation tricks can be applied to simplify the right-side expression:\n\nSince, all \\(h_i\\)’s are independent, we can write \\(p( \\textbf{h | v} ) = \\prod_{k=1}^n p(h_k|\\textbf{v})\\).\n\\(\\sum_h\\) can be rewritten as \\(\\sum_{h_i} \\sum_{\\textbf{h}_{\\textbf{-i}}}\\), where \\(\\textbf{h}_{\\textbf{-i}}\\) represents a vector without the ith node.\nLastly, we can rewrite \\(\\prod_{k=1}^n p(h_k|\\textbf{v}) = p(h_i| \\textbf{v}) p(\\textbf{h}_{\\textbf{-i}} | \\textbf{v})\\).\n\\(\\sum_{\\textbf{h}_{\\textbf{-i}}}  p(\\textbf{h}_{\\textbf{-i}} | \\textbf{v}) = 1\\), obviously!\n\nUsing the above tricks, the equation becomes:\n\\[\\begin{equation*}\n\\begin{split}\n\n-\\sum_\\textbf{h} p( \\textbf{h | v} ) h_i v_j &= \\sum_{h_i} \\sum_{\\textbf{h}_{\\textbf{-i}}} p(h_i| \\textbf{v}) p(\\textbf{h}_{\\textbf{-i}} | \\textbf{v}) h_i v_j\n\\\\\n&= \\sum_{h_i} p(h_i| \\textbf{v}) h_i v_j \\sum_{\\textbf{h}_{\\textbf{-i}}}  p(\\textbf{h}_{\\textbf{-i}} | \\textbf{v})\n\\\\\n&= \\sum_{h_i} p(h_i| \\textbf{v}) h_i v_j\n\\\\\n&= p(H_i=0| \\textbf{v}) (0) v_j + p(H_i=1| \\textbf{v}) (1) v_j\n\\\\\n&= p(H_i=1| \\textbf{v}) v_j\n\\\\\n&= \\sigma \\Big(\\sum_{j=1}^m w_{ij}v_j + c_i \\Big) v_j\n\n\\end{split}\n\\end{equation*}\\]"
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#second-term-simplification",
    "href": "posts/restricted_boltzmann_machines/index.html#second-term-simplification",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "Second Term Simplification",
    "text": "Second Term Simplification\n\\[\\begin{equation*}\n\\begin{split}\n\n\\sum_{\\textbf{v},\\textbf{h}} p(\\textbf{v},\\textbf{h}) \\frac{\\partial E(\\textbf{v},\\textbf{h})}{\\partial w_{ij}}\n&= -\\sum_\\textbf{v} p(\\textbf{v})\n\\sum_\\textbf{h} p( \\textbf{h | v} ) h_i v_j\n\n\\\\\n\n&= -\\sum_\\textbf{v} p(\\textbf{v})\np(H_i=1| \\textbf{v}) v_j\n\\end{split}\n\\end{equation*}\\]\nHence, the final gradient comes out to be:\n\\[\n\\frac{\\partial\\mathcal{L}(\\theta|\\textbf{v})}{\\partial w_{ij}}\n=\np(H_i=1| \\textbf{v}) v_j\n-\n\\sum_\\textbf{v} p(\\textbf{v})\np(H_i=1| \\textbf{v}) v_j\n\\]"
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#approx.-1",
    "href": "posts/restricted_boltzmann_machines/index.html#approx.-1",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "Approx. 1",
    "text": "Approx. 1\nInstead of looping over \\(\\textbf{v}\\), we sample some samples from the distribution at hand [\\(p(\\textbf{v}, \\textbf{h})\\) in this case] and calculate their mean. Practically, using just one sample works fine and fairly approximates the expectation.\n\\[\n\\hat{\\textbf{v}} \\sim p(\\textbf{v}, \\textbf{h})\n\\]\n\\[\n\\frac{\\partial\\mathcal{L}(\\theta|\\textbf{v})}{\\partial w_{ij}}\n=\np(H_i=1| \\textbf{v}) v_j\n-\np(H_i=1| \\hat{\\textbf{v}}) \\hat{v}_j\n\\]\n\\[\n\\frac{\\partial\\mathcal{L}(\\theta|\\textbf{v})}{\\partial b_{j}} = v_j - \\hat{v}_j\n\\]\n\\[\n\\frac{\\partial\\mathcal{L}(\\theta|\\textbf{v})}{\\partial b_{j}}\n=\np(H_i=1| \\textbf{v})\n-\np(H_i=1| \\hat{\\textbf{v}})\n\\]"
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#approx.-2",
    "href": "posts/restricted_boltzmann_machines/index.html#approx.-2",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "Approx. 2",
    "text": "Approx. 2\nStill, sampling \\(\\hat{\\textbf{v}}\\) from \\(p(\\textbf{v}, \\textbf{h})\\) is not possible because of the intractable \\(Z\\) term encountered in the calculation of \\(p(\\textbf{v}, \\textbf{h})\\). However, the good news is that it is relatively (very!) easy to calculate \\(p(\\textbf{h} | \\textbf{v})\\) and \\(p(\\textbf{v} | \\textbf{h})\\) (remember the Bernoulli distributions we described a while ago?). Here’s where the Gibbs sampler comes in handy.\nBasically, according to Mr. Gibbs, if you start with some \\(\\textbf{v}^0\\) AND sample \\(\\textbf{h}^1 \\sim p(\\textbf{h} | \\textbf{v}^0)\\) and \\(\\textbf{v}^1 \\sim p(\\textbf{v} | \\textbf{h}^1)\\) AND keep going till convergence (suppose we reach convergence after \\(k\\) iterations), the samples \\(\\textbf{h}^k\\) and \\(\\textbf{v}^k\\) will look as if they were sampled from the intractable joint distribution \\(p(\\textbf{v}, \\textbf{h})\\).\nWe use Gibbs sampling because we can easily sample from \\(p(\\textbf{h}|\\textbf{v})\\) and \\(p(\\textbf{v}|\\textbf{h})\\) which are simple Bernoulli distributions."
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#approx.-3",
    "href": "posts/restricted_boltzmann_machines/index.html#approx.-3",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "Approx. 3",
    "text": "Approx. 3\nA small problem persists — performing Gibbs sampling until convergence is computationally expensive. However, the good news is that performing just one step of Gibbs sampling approximates the sampling process fairly enough for the gradient descent algorithm to work."
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#rbms",
    "href": "posts/restricted_boltzmann_machines/index.html#rbms",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "RBMs",
    "text": "RBMs\n\nAn Introduction to RBMs by Fischer and Igel\nHinton’s “A Practical Guide to Training RBMs”\nAn introductory Medium blog"
  },
  {
    "objectID": "posts/restricted_boltzmann_machines/index.html#gibbs",
    "href": "posts/restricted_boltzmann_machines/index.html#gibbs",
    "title": "Bernoulli Restricted Boltzmann Machines",
    "section": "Gibbs",
    "text": "Gibbs\n\nA short intro to Gibbs sampling\nA detailed intro to Gibbs sampling"
  },
  {
    "objectID": "posts/intro_rl/index.html",
    "href": "posts/intro_rl/index.html",
    "title": "The RL Ideology",
    "section": "",
    "text": "The good thing about RL is that it possesses the power to produce exceptional results which can’t be made by a human. It’s the way to create an alternate intelligence which works in different but better ways than a human. On the other hand, supervised learning focuses on replicating the best behaviours of humans, which might not be the best on an absolute scale but is still helpful and beautiful.\n\nSome Important Terminologies\nBorrowed from Sergey Levine’s Deep RL course, the below slides do a great job of getting the terminologies clear. Have a good look at them:\n\n\n\nRL vs Supervised Learning\n\n\n\n\n\nBasic RL Terminologies\n\n\nNote that observation and state are two different things. A good analogy would be to consider state as the detailed memory layout of a computer and the observation as the photo of the computer screen. It is possible to get the observation from the state but not the other way around.\nAlos, the above diagrams shows the “Markov Property”, which means that the current state is enough to predict the future. However, this does not guarantee that the future is deterministic. It may be stochastic but knowing the past doesn’t really help reduce that randomness.\n\n\nBehavioural Cloning\nIt belongs to a category of learning algorithms called “imitation learning” which basically refers to learning a policy which imitates human decisions. Behavioural cloning is a subset of this branch which studies how we can take the “supervised” approach to learning a policy. The data is tuples \\((o_t, a_t)\\) which can be acquired through humans (eg. hours of recordings of human driving). Once that is done, the policy (which will be a CNN in case of the self-driving problem) can be learned in the same way a classifier learns based on labelled data.\n\nDoes it work?\nNo. Because of a very fundamental modelling issue. We are training only on the “correct” samples. Our data doesn’t consists of “mistakes” and corresponding “corrections”. This is a problem because our policy might not be perfect and may predict incorrectly sometimes. This may not be a problem in supervised learning scenarios, but in policy-learning, wrong predictions put the model in unseen states, resulting in a wrong (or wronger!) prediction again. This error keeps compounding, which gives us a trajectory very different from the one we expected.\n\n\n\nSolutions\n\nCleverly collect and augment your data. For eg, when training a self-driving model, some NVIDIA researchers used a smart trick to induce “mistakes” and “corrections” to the data. They equipped the care with a left and a right camera along with the central camera and labelled all the frames from the left camera with a “right” action and vice-versa. Too good, isn’t it?\nWhat if we design a model which does very less mistakes, in the beginning too? This way, the model won’t find itself in unseen scenarios. First of all, what are the problems which prevent us from making a very good model?\n\nMarkovian assumption is a strong assumption. Humans are never markovian. Hence, the solution is to supply a few previous states along with the current state to the policy model. Easy, isn’t it? Now, there’s a small warning. This might not always help. Some information in the previous frames might be extensively correlated with future frames, confusing the model. I didn’t understand this fully, so more on this later.\nIf the data is multimodal, we might average out different modes leading to a bad prediction. The solution is to use more expressive continuous distributions. There are mainly 3 options:\nA. Gaussian Mixture Models - The outputs of the neural network will be the means, variances and gaussian weights. Then, we can just maximise the log of the probability calculated using those outputs. Simple, but not as powerful as the other too!\nB. Latent Variable Models - Take the conditional VAE as an example. More on this in the later lectures. For now, keep in mind that the random number added to the input chooses the mode which should be outputted.\nC. Diffusion Models - Here, we learn a network which predicts a slightly better action given a bad action. This is repeated and we get a good action. Note that this model is responsible for outputting an action based on the given state.\n\nSuppose we want to train a self-driving car to go to location p1. The naive way is to train a policy \\(\\pi_\\theta (a|s)\\) on data in which the expert drives to the location \\(p_1\\). But, the idea is that if we train a policy \\(\\pi_\\theta (a|s, g)\\) on data which contains samples having different target locations instead of just one. It turns out the second way gives much better performance on the task of reaching location \\(p_1\\), because it’s training data contains a lot of mistakes and corrections which are helpful. This is how multitask learning can help.\nWait, wait, wait. Instead of tinkering with policy, what if we change the data collection procedure so that it contains the unfamiliar territories which are problematic for our policy? That’s the DAgger algorithm. It’s simple yet elegant.\n\n\n\nDAgger Algorithm\n\n\n\n\n\n\nWhy need RL then? What’s the goal of RL?\nSo, the thing with IL is that the decisions are based on “current” rewards, not “latent” rewards. This is not how humans work. Humans take decisions such that the reward after a few actions is high, which involves planning. So, the goal of RL is design a policy which takes decisions which might not lead to immediate rewards but may turn out to be rewarding after some actions. Basically, we want to give “farsightedness” to a policy and that can’t be done through IL.\n\nTo be more precise, the goal of RL is to design a policy which provides the highest expectation (over multiple trajectories) of the sum of the reward obtained at each state. This is how we induce “planning” or “farsightedness” in a model.\nNOTE: In RL, we always care about expectations of rewards. This allows us to perform gradient descent because the expectations are always smooth, even though the rewards are discrete. For example, if we want to design a self-driving car based on the reward function which gives -1 if the car falls off the road and +1 otherwise. Here, the reward is discrete but its expectation is continuous if we model the reward using a Bernoulli distribution. Hence, we train a policy which maximises the “expected” reward rather than the reward itself, so that gradient descent-based learning is possible.\n\n\nAnatomy of an RL Algorithm\n\n\n\nEvaluating the RL policy\nSo, as discussed above, we want to find a policy which maximises the expected sum of rewards over the trajectories. In other words, we want to find a policy, which, if used to select the next action, usually gives us a high value when we add the rewards obtained after each action. In mathematical terms, our target function is as follows:\n\\[\nE_{\\tau \\sim p_\\theta(\\tau)} \\bigg[ \\sum_t r(s_t, a_t) \\bigg]\n\\]\nBut, how to calculate it? We realise that it is difficult to calculate when we rewrite the above expression as:\n\\[\nE_{s_1 \\sim p(s_1)}\\bigg[ E_{a_1 \\sim \\pi(a_1|s_1)} \\bigg[r(s_1, a_1) + E_{s_2 \\sim p(s_2|s_1, a_1)}\\big[ E_{a_2 \\sim \\pi(a_2|s_2)} \\big[r(s_2, a_2) + \\ldots \\big] \\big] \\bigg] \\bigg]\n\\]\nSo many expectations, right? This definitely needs to be simplified. Let’s introduce two abstractions which help us simplify the calculation of this term.\n\n1. Q-Function - \\(Q^{\\pi_\\theta} (s_t, a_t)\\)\nSuppose you are at a state \\(s_t\\) and you take an action \\(a_t\\) and you are following the policy \\(\\pi_\\theta\\). Then, if you keep doing actions according to the policy \\(\\pi\\) till you reach the terminal state \\(s_T\\). Now, if you somehow calculate the expected reward at each state and sum all of them up, you get the Q-value of the tuple \\((s_t, a_t)\\). In other words, this fancy-named function stands for the sum of expected rewards at each state, given that you start from \\(s_t\\) by taking an action \\(a_t\\) and keep making decisions according to a policy \\(\\pi_\\theta\\). In mathematical terms,\n\\[\nQ^{\\pi_\\theta}(s_t, a_t) = \\sum_{t'=t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t, a_t]\n\\]\nIntuitively speaking, the Q-value of \\((s_t, a_t)\\) represents the “quality” of taking an action \\(a_t\\) at a state \\(s_t\\).\n(Note that we haven’t talked about the difficulties in calculating Q-values.)\n\n\n2. Value Function - \\(V^{\\pi_\\theta}(s_t)\\)\nNow, if you understand the meaning of the Q-function, understanding value function is easy. Intuitively speaking, the value of a state \\(s_t\\) represents the importance of that state, from a reward perspective. It helps us evaluate how “good” the state is. This is very important because once we find a way to get the “value” of a state, designing an good policy becomes very easy because the policy just needs to choose actions which lead to “good-value” states! In mathematical terms, we can write it as:\n\\[\nV^{\\pi_\\theta}(s_t) = \\sum_{t'=t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t]\n\\]\nThis expression can be further simplified if we utilised the Q-function:\n\\[\nV^{\\pi_\\theta}(s_t) = E_{a_t \\sim \\pi_\\theta(a_t|s_t)} [Q^{\\pi_\\theta}(s_t, a_t)]\n\\]\nOne more way to interpret the value function is that it is the average of the qualities of all the actions that can be taken from state \\(s_t\\).\nEasy, isn’t it? So, now these abstractions are clear. But how can we use them to simplify our RL target function? If you look closely, it is easy to notice that:\n\\[\nE_{\\tau \\sim p_\\theta(\\tau)} \\bigg[ \\sum_t r(s_t, a_t) \\bigg] = E_{s_1 \\sim p(s_1)} V^{\\pi_\\theta}(s_1)\n\\]\n\n\n\nImproving the policy based on evaluation\nSo, now we have the tools to evaluate the quality of an action (i.e. Q-function) and to evaluate the “goodness” of a state (Value function). Great! Now the question is, “how can we use these tools to modify the policy?”. It turns out that there are two ideas:\n\nIdea 1: “I want my agent to always take the “best” action!” (More exploitation)\nDesign a new policy \\(\\pi_\\theta'\\) such that \\(\\pi_\\theta'(a^*|s)=1\\), where \\(a^* = \\arg \\max_aQ^{\\pi_\\theta}(s, a)\\). Basically, the new policy should always take the action which had the highest quality according to the previous policy.\n\n\nIdea 2: “I want my agent to select one out of some above-average actions!” (More exploration!)\nDesign a new policy \\(\\pi_\\theta'\\) such that \\(\\pi_\\theta'(\\hat a|s)&gt; \\pi_\\theta(\\hat a|s) \\text{ if } Q^{\\pi_\\theta}(s, a)&gt;V^{\\pi_\\theta}(s)\\). In other words, we can improve the policy by increasing the probability of selection of actions which were “above-average”.\nThat’s all for now. We will go into improving the policy in the next blog. Thanks for reading! Hope you liked it. If you find any mistakes or inaccuracies, feel free to send an email to shah.15@iitj.ac.in.\n(Credits: Slides borrowed from the amazing Deep RL course by Sergey Levine)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rushi's Furnace",
    "section": "",
    "text": "Kolmogorov Complexity and “Sophistication”\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Policy Gradient\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe RL Ideology\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli Restricted Boltzmann Machines\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nAn Expansive Probability Refresher\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2024\n\n\n11 min\n\n\n\n\n\n\nNo matching items"
  }
]